# Device Settings - Auto-detected, can be overridden
USE_CUDA=auto  # auto, true, or false
DEVICE=auto    # auto, cpu, or cuda

# Model Paths
CLAIRE_MODEL_Q4_PATH=models/claire_v1.0.0_q4_k_m.gguf
CLAIRE_MODEL_F16_PATH=models/claire_v1.0.0_f16.gguf
LANGUAGE_MODEL_PATH=models/distilbert_language.pt
EMOTION_MODEL_PATH=models/distilbert_emotion.pt

# Model Selection (auto-selected based on device)
AUTO_SELECT_MODEL=true

# GGUF Model Settings
MODEL_CONTEXT_SIZE=2048
MODEL_MAX_TOKENS=1024
MODEL_TEMPERATURE=0.3
MODEL_TOP_P=0.9
MODEL_REPEAT_PENALTY=1.1
MODEL_N_BATCH=512      # For GPU
MODEL_N_BATCH_CPU=256  # For CPU

# GPU Settings
GPU_LAYERS=35  # Number of layers to offload to GPU (0 for CPU-only)

# CPU Optimization
OMP_NUM_THREADS=auto  # auto = use all cores
MKL_NUM_THREADS=auto
LLAMA_CPP_THREADS=auto
USE_MMAP=true
USE_MLOCK=false
F16_KV_CPU=false  # Use fp32 for KV cache on CPU (faster)

# API Settings
API_V1_STR=/api/v1
PROJECT_NAME=CLAIRE-RAG [BACKEND]
VERSION=1.0.0

# Embedding Settings
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
MAX_LENGTH=512
TOP_K=4

# Timeout Settings (seconds)
REQUEST_TIMEOUT=300
OCR_TIMEOUT=30
MODEL_INFERENCE_TIMEOUT=120  # For GPU
MODEL_INFERENCE_TIMEOUT_CPU=300  # For CPU
VECTOR_SEARCH_TIMEOUT=30
GENERATION_TIMEOUT_COOLDOWN=60
MAX_FILE_SIZE=5242880  # 5MB

# Response Settings
MAX_RESPONSE_LENGTH=1000
SHORT_MESSAGE_THRESHOLD=20  # Skip model for messages shorter than this

# Cache Settings
ENABLE_MODEL_CACHE=true
USE_CACHE=true

# Worker Settings
MAX_WORKERS=2
BATCH_SIZE=1

# Greeting Detection
ENABLE_GREETING_DETECTION=true